(window.webpackJsonp=window.webpackJsonp||[]).push([[25],{397:function(t,s,a){"use strict";a.r(s);var n=a(46),p=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"深度学习算法和模型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#深度学习算法和模型"}},[t._v("#")]),t._v(" 深度学习算法和模型")]),t._v(" "),a("h2",{attrs:{id:"主成分分析-principal-component-analysis-pca"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#主成分分析-principal-component-analysis-pca"}},[t._v("#")]),t._v(" 主成分分析(Principal Component Analysis，PCA)")]),t._v(" "),a("p",[t._v("无监督的特征降维技术，通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量，转换后的这组变量叫主成分。")]),t._v(" "),a("ol",[a("li",[t._v("计算每个特征的均值并减去均值")]),t._v(" "),a("li",[t._v("求散布矩阵")]),t._v(" "),a("li",[t._v("计算散布矩阵的特征值和特征向量")]),t._v(" "),a("li",[t._v("按照特征值从大到小排序，选择最大的k个特征值对应的特征向量")]),t._v(" "),a("li",[t._v("将样本点投影到选取的特征向量上")])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("pca")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# k是降维数")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算每个特征的均值")]),t._v("\n    n_samples"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_features "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n    mean "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_features"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 减去均值")]),t._v("\n    norm_X "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" X "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" mean\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 求散布矩阵")]),t._v("\n    scatter_matrix "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transpose"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("norm_X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" norm_X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算散布矩阵的特征值和特征向量")]),t._v("\n    eig_val"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" eig_vec "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linalg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("eig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("scatter_matrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    eig_pairs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("abs")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("eig_val"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" eig_vec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_features"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 按照特征值从大到小排序")]),t._v("\n    eig_pairs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sort"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("reverse"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 选择最大的k个特征值对应的特征向量")]),t._v("\n    feature "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("ele"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" ele "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" eig_pairs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("k"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将样本点投影到选取的特征向量上")]),t._v("\n    data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("norm_X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transpose"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" data\n")])])]),a("h3",{attrs:{id:"线性判别分析-linear-discriminant-analysis-lda"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#线性判别分析-linear-discriminant-analysis-lda"}},[t._v("#")]),t._v(" 线性判别分析(Linear Discriminant Analysis，LDA)")]),t._v(" "),a("p",[t._v("LDA一种有监督的线性降维方法，也就是说它的数据集的每个样本是有类别输出的。这点和PCA不同，PCA是不考虑样本类别输出的无监督降维技术。核心思想是往线性判别超平面的法向量上投影，使的区分度最大（高内聚，低耦合）。LDA是为了使得降维后的数据点尽可能地容易被区分！用一句话概括，就是投影后类内方差最小，类间方差最大。")]),t._v(" "),a("ol",[a("li",[t._v("对d维数据进行标准化处理（d为特征数量）")]),t._v(" "),a("li",[t._v("对每一类别，计算d维的均值向量")]),t._v(" "),a("li",[t._v("构造类间的散步矩阵和类内的散步矩阵")]),t._v(" "),a("li",[t._v("计算矩阵的特征值和对应的特征向量")]),t._v(" "),a("li",[t._v("选取前k个特征值对应的特征向量，构造一个d x k维的转换矩阵W，特征向量以列的形式排列")]),t._v(" "),a("li",[t._v("使用转换矩阵W将样本映射到新的特征子空间上")])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("lda")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" target"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_dim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    :param data: (n_samples, n_features)\n    :param target: 数据类别\n    :param n_dim: 目标维数\n    :return: (n_samples, n_dims)\n    """')]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类别数")]),t._v("\n    clusters "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unique"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("target"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# LDA降维最多降到类别数k-1的维数")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" n_dim "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("clusters"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"K is too much"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"please input again"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        exit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类内散布矩阵")]),t._v("\n    Sw "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" clusters"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        datai "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("target "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        datai "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" datai "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" datai"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        Swi "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("datai"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("datai"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        Sw "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" Swi\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类间散布矩阵")]),t._v("\n    SB "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    u "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 所有样本的平均值")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" clusters"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        Ni "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("target "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        ui "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("target "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 某个类别的平均值")]),t._v("\n        SBi "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Ni "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ui "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ui "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        SB "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" SBi\n\n    S "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linalg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("inv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Sw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" SB\n    eigVals"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" eigVects "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linalg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("eig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("S"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 求特征值，特征向量")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 选取前 n_dim 个特征值对应的特征向量，构造一个 d x n_dim 维的转换矩阵w，特征向量以列的形式排列")]),t._v("\n    eigValInd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("argsort"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("eigVals"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    eigValInd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" eigValInd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("n_dim "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    w "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" eigVects"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" eigValInd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用转换矩阵W将样本映射到新的特征子空间上")]),t._v("\n    data_ndim "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" w"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" data_ndim\n")])])]),a("h3",{attrs:{id:"自编码器-autoencoder-ae"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#自编码器-autoencoder-ae"}},[t._v("#")]),t._v(" 自编码器(autoencoder, AE)")]),t._v(" "),a("p",[t._v("自编码器是一类在半监督学习和非监督学习中使用的人工神经网络，其功能是通过将输入信息作为学习目标，对输入信息进行表征学习。自编码器包含编码器（encoder）和解码器（decoder）两部分。按学习范式，自编码器可以被分为收缩自编码器、正则自编码器和变分自编码器，其中前两者是判别模型、后者是生成模型。按构筑类型，自编码器可以是前馈结构或递归结构的神经网络。自编码器具有一般意义上表征学习算法的功能，被应用于降维和异常值检测。包含卷积层构筑的自编码器可被应用于计算机视觉问题，包括图像降噪、神经风格迁移等。")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("AutoEncoder")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("AutoEncoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encoder "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Tanh"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 双曲正切，其输出的数值范围为-1到1")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Tanh"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Tanh"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 压缩到3维以便用plt可视化")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decoder "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Tanh"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Tanh"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Tanh"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sigmoid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 压缩到0-1之间")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        encoded "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        decoded "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("encoded"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" encoded"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" decoded\n")])])]),a("h2",{attrs:{id:"批量梯度下降法bgd"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#批量梯度下降法bgd"}},[t._v("#")]),t._v(" 批量梯度下降法bgd")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("BGD_LR")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" alpha"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" maxepochs"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" epsilon"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e-4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    xMat "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    yMat "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("n "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" xMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n    weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#初始化模型参数")]),t._v("\n    epochs_count "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n    loss_list "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    epochs_list "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" epochs_count "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" maxepochs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        loss "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cost"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("yMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#上一次损失值")]),t._v("\n        hypothesis "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sigmoid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#预测值")]),t._v("\n        error "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" hypothesis "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("yMat "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#预测值与实际值误差")]),t._v("\n        grad "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#损失函数的梯度")]),t._v("\n        last_weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" weights "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#上一轮迭代的参数")]),t._v("\n        weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" alpha"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("grad "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#参数更新")]),t._v("\n        loss_new "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cost"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("yMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#当前损失值")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss_new"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("abs")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss_new"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("epsilon"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#终止条件")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("break")]),t._v("\n        loss_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss_new"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        epochs_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epochs_count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        epochs_count "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'迭代到第{}次，结束迭代！'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epochs_count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    plt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("plot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epochs_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("loss_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    plt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xlabel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'epochs'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    plt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ylabel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'loss'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    plt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" weights\n")])])]),a("h2",{attrs:{id:"随机梯度下降法sgd"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#随机梯度下降法sgd"}},[t._v("#")]),t._v(" 随机梯度下降法sgd")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("SGD_LR")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" alpha"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" maxepochs"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("epsilon"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e-4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    xMat "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    yMat "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" xMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n    weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 模型参数")]),t._v("\n    epochs_count "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n    loss_list "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    epochs_list "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" epochs_count "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" maxepochs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        rand_i "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("randint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 随机取一个样本")]),t._v("\n        loss "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cost"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("yMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#前一次迭代的损失值")]),t._v("\n        hypothesis "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sigmoid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("rand_i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#预测值")]),t._v("\n        error "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" hypothesis "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("yMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("rand_i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#预测值与实际值误差")]),t._v("\n        grad "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("rand_i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#损失函数的梯度")]),t._v("\n        weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" alpha"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("grad "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#参数更新")]),t._v("\n        loss_new "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cost"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("yMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#当前迭代的损失值")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss_new"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("abs")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss_new"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("epsilon"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("break")]),t._v("\n        loss_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss_new"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        epochs_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epochs_count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        epochs_count "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'迭代到第{}次，结束迭代！'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epochs_count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    plt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("plot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epochs_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("loss_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    plt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xlabel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'epochs'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    plt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ylabel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'loss'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    plt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" weights\n")])])]),a("h2",{attrs:{id:"小批量梯度下降法mbgd"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#小批量梯度下降法mbgd"}},[t._v("#")]),t._v(" 小批量梯度下降法mbgd")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("MBGD_LR")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" alpha"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" maxepochs"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("epsilon"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e-4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    xMat "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    yMat "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" xMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n    weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 模型参数")]),t._v("\n    epochs_count "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n    loss_list "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    epochs_list "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" epochs_count "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" maxepochs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        randIndex "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choice"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batch_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" replace"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        loss "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cost"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("yMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#前一次迭代的损失值")]),t._v("\n        hypothesis "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sigmoid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("randIndex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#预测值")]),t._v("\n        error "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" hypothesis "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("yMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("randIndex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#预测值与实际值误差")]),t._v("\n        grad "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("randIndex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#损失函数的梯度")]),t._v("\n        weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" alpha"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("grad "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#参数更新")]),t._v("\n        loss_new "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cost"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("yMat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#当前迭代的损失值")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss_new"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("abs")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss_new"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("epsilon"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#终止条件")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("break")]),t._v("\n        loss_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss_new"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        epochs_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epochs_count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        epochs_count "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'迭代到第{}次，结束迭代！'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epochs_count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    plt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("plot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epochs_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("loss_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    plt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xlabel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'epochs'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    plt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ylabel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'loss'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    plt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" weights\n")])])]),a("h2",{attrs:{id:"人工神经网络ann"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#人工神经网络ann"}},[t._v("#")]),t._v(" 人工神经网络ann")]),t._v(" "),a("p",[t._v("人工神经网络（Artificial Neural Network，即ANN ），是20世纪80 年代以来人工智能领域兴起的研究热点。它从信息处理角度对人脑神经元网络进行抽象， 建立某种简单模型，按不同的连接方式组成不同的网络。在工程与学术界也常直接简称为神经网络或类神经网络。神经网络是一种运算模型，由大量的节点（或称神经元）之间相互联接构成。每个节点代表一种特定的输出函数，称为激励函数（activation function）。每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为权重，这相当于人工神经网络的记忆。网络的输出则依网络的连接方式，权重值和激励函数的不同而不同。而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。")]),t._v(" "),a("h2",{attrs:{id:"前馈神经网络fnn"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#前馈神经网络fnn"}},[t._v("#")]),t._v(" 前馈神经网络fnn")]),t._v(" "),a("p",[t._v("在前馈神经网络中，各神经元分别属于不同的层。每一层的神经元可以接收前一层神经元的信号，并产生信号输出到下一层。第 0 层叫输入层，最后一层叫输出层，其它中间层叫做隐藏层，相邻两层的神经元之间为全连接关系，也称为全连接神经网络(FNN)，表现形式如下图所示。\n"),a("img",{attrs:{src:"/blog/img/fnn.png",alt:""}})]),t._v(" "),a("h3",{attrs:{id:"常见前馈神经网络"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#常见前馈神经网络"}},[t._v("#")]),t._v(" 常见前馈神经网络")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("感知器网络")]),t._v("\n感知器（又叫感知机）是最简单的前馈网络，它主要用于模式分类，也可用在基于模式分类的学习控制和多模态控制中。感知器网络可分为单层感知器网络和多层感知器网络。")]),t._v(" "),a("li",[a("strong",[t._v("BP网络")]),t._v("\nBP网络是指连接权调整采用了反向传播（Back Propagation）学习算法的前馈网络。与感知器不同之处在于，BP网络的神经元变换函数采用了S形函数（Sigmoid函数），因此输出量是0~1之间的连续量，可实现从输入到输出的任意的非线性映射。")]),t._v(" "),a("li",[a("strong",[t._v("RBF网络")]),t._v("\nRBF网络是指隐含层神经元由RBF神经元组成的前馈网络。RBF神经元是指神经元的变换函数为RBF（Radial Basis Function，径向基函数）的神经元。典型的RBF网络由三层组成：一个输入层，一个或多个由RBF神经元组成的RBF层（隐含层），一个由线性神经元组成的输出层。")])]),t._v(" "),a("h2",{attrs:{id:"循环神经网络rnn"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#循环神经网络rnn"}},[t._v("#")]),t._v(" 循环神经网络rnn")]),t._v(" "),a("p",[t._v("循环神经网络（Recurrent Neural Network, RNN）是一类以序列（sequence）数据为输入，在序列的演进方向进行递归（recursion）且所有节点（循环单元）按链式连接的递归神经网络（recursive neural network）\n"),a("img",{attrs:{src:"/blog/img/rnn.png",alt:""}})]),t._v(" "),a("h2",{attrs:{id:"卷积神经网络cnn"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#卷积神经网络cnn"}},[t._v("#")]),t._v(" 卷积神经网络cnn")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("数据输入层（Input Layer）")]),t._v("\n灰度化\n归一化")]),t._v(" "),a("li",[a("strong",[t._v("卷积层（Convolutional Layer）")]),t._v("\n卷积核\n卷积核的深度\n卷积运算\n步幅\n填充")]),t._v(" "),a("li",[a("strong",[t._v("激活层（Relu Layer）")]),t._v("\nrelu函数\nsigmoid函数\ntanh函数")]),t._v(" "),a("li",[a("strong",[t._v("池化层（Pooling Layer）")]),t._v("\n最大池化\n平均池化")]),t._v(" "),a("li",[a("strong",[t._v("全连接层（FC Layer）")])])]),t._v(" "),a("h2",{attrs:{id:"logistic回归与softmax回归"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#logistic回归与softmax回归"}},[t._v("#")]),t._v(" logistic回归与softmax回归")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("logistic回归")]),t._v("\nlogistic回归实际上就是加了个sigmoid函数的线性回归，这个sigmoid函数的好处就在于，将结果归到了0到1这个区间里面了，并且sigmoid(0)=0.5，也就是说里面的线性部分的结果大于零小于零就可以直接计算到了。")]),t._v(" "),a("li",[a("strong",[t._v("softmax回归")]),t._v("\nsoftmax逻辑回归模型是logistic回归模型在多分类问题上的推广，在多分类问题中，类标签y可以取两个以上的值。")])]),t._v(" "),a("h2",{attrs:{id:"支持向量机svm"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#支持向量机svm"}},[t._v("#")]),t._v(" 支持向量机svm")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("hard margin svm(最大间隔分类器)")]),t._v(" "),a("img",{attrs:{src:"/blog/img/svm.png",alt:""}})]),t._v(" "),a("li",[a("strong",[t._v("soft margin svm")])]),t._v(" "),a("li",[a("strong",[t._v("kernel svm")])])]),t._v(" "),a("h2",{attrs:{id:"混淆矩阵"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#混淆矩阵"}},[t._v("#")]),t._v(" 混淆矩阵")]),t._v(" "),a("p",[t._v("TP\nTN\nFP\nFN")]),t._v(" "),a("h2",{attrs:{id:"正则化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#正则化"}},[t._v("#")]),t._v(" 正则化")]),t._v(" "),a("p",[t._v("L1正则化\nL2正则化")]),t._v(" "),a("h2",{attrs:{id:"kmeans聚类算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#kmeans聚类算法"}},[t._v("#")]),t._v(" kmeans聚类算法")]),t._v(" "),a("ol",[a("li",[t._v("首先确定一个k值，即我们希望将数据集经过聚类得到k个集合。")]),t._v(" "),a("li",[t._v("从数据集中随机选择k个数据点作为质心。")]),t._v(" "),a("li",[t._v("对数据集中每一个点，计算其与每一个质心的距离（如欧式距离），离哪个质心近，就划分到哪个质心所属的集合。")]),t._v(" "),a("li",[t._v("把所有数据归好集合后，一共有k个集合。然后重新计算每个集合的质心。")]),t._v(" "),a("li",[t._v("如果新计算出来的质心和原来的质心之间的距离小于某一个设置的阈值（表示重新计算的质心的位置变化不大，趋于稳定，或者说收敛），我们可以认为聚类已经达到期望的结果，算法终止。")]),t._v(" "),a("li",[t._v("如果新质心和原质心距离变化很大，需要迭代3~5步骤。")])])])}),[],!1,null,null,null);s.default=p.exports}}]);